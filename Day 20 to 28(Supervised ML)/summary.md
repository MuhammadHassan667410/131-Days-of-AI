#  Day 20 to 28 - Supervised Machine Learning Summary

Welcome to Days 20 to 28 of my 131-day AI learning journey!  
This section focuses on Supervised Machine Learning — using labeled data to make predictions.

---

##  Daily Breakdown

###  Day 20: Introduction to Machine Learning
- Supervised vs Unsupervised Learning
- ML vs Traditional Programming
- Real-world applications (spam filters, price prediction, etc.)

###  Day 21: Linear Regression
- Model: `y = mx + b` (weight & bias)
- MSE (Mean Squared Error), R² Score
- Training/test split using `train_test_split()`

###  Day 22: Polynomial Regression
- Fitting curves with `PolynomialFeatures`
- Overfitting vs Underfitting
- Visualization of curve complexity

###  Day 23: K-Nearest Neighbors (KNN)
- Classification using distance
- Importance of `k` value
- Decision boundary visualization

###  Day 24: Logistic Regression
- Binary classification using sigmoid
- Output is probability (0–1)
- Log loss function (cross-entropy)

###  Day 25: Decision Trees
- Tree splits using Entropy or Gini
- Interpretable structure: root, internal, leaf nodes
- `plot_tree()` from `sklearn.tree`

###  Day 26: Visualization Practice
- KNN decision boundaries (2D)
- Visualizing tree splits and depth
- Compared models on same dataset

###  Day 27: Model Evaluation Metrics
- Accuracy, Confusion Matrix
- Precision, Recall, F1 Score
- `classification_report()` from sklearn

###  Day 28: Supervised Learning Mini Project, SVM model
- Dataset: Iris or Titanic
- Applied Logistic Regression, KNN, Decision Tree
- Preprocessing, training, evaluation, visualization

---

##  Key Skills Learned
- Model training & testing with `scikit-learn`
- Choosing metrics based on task
- Visualization of results and decision boundaries
- Understanding overfitting & model complexity

---

## ⏭ What’s Next: Day 29+
➡️ Start **Unsupervised Learning**:
- K-Means Clustering
- PCA
- Real-world use cases like grouping & anomaly detection
